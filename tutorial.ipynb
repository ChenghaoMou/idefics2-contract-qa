{
 "cells": [
  {
   "cell_type": "raw",
   "id": "20a8974b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Fine-tuning Idefics2-8B for Multi-page Contract VQA\"\n",
    "author: \"Chenghao Mou\"\n",
    "date: \"June 30, 2024\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e45a81442475c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "[Idefics2](https://huggingface.co/HuggingFaceM4/idefics2-8b) is a powerful vision language model from Huggingface that has been pre-trained and fine-tuned to take arbitrary text and image input and generate text output, with improved performance on visual question and answering and OCR over the first generation. It is released under Apache 2.0, along with its multimodal instruction fine-tuning dataset [The Cauldron](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron).\n",
    "\n",
    "In this notebook, I am going to fine-tune the model for multi-page contract document QA. For a while, document QA can only been done with single-page documents, while in reality, one could have 100+ pages in a single agreement. With that being said, it is still difficult for this model to support beyond 10 pages with a GPU-poor man's budget. Some features that are supported by Idefics2, such as sub-image tokenization (64 * 5 = 320 tokens for one page instead of 64) and high resolution (980 * 980) are also disabled for my experiment due to the hardware constraints. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a0deb85880413",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0945b38d36319e0",
   "metadata": {},
   "source": [
    "### Load the Raw Data\n",
    "\n",
    "I have been collecting public contract dataset for a while. `chenghao/sec-material-contracts` is an ongoing project that crawls contract documents from the [EDGAR](sec.gov) website. It includes both the raw html contract content and all relevant metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ad1e8aa1c1e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import io\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import fitz\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from trafilatura import html2txt\n",
    "from weasyprint import HTML\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_repo = \"chenghao/sec-material-contracts\"\n",
    "dataset = load_dataset(source_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af67a62ecfc9defb",
   "metadata": {},
   "source": [
    "### Convert HTML to PDF and Images\n",
    "\n",
    "We want the model to be able to read the document images and answer the query accordingly. This means we need to convert the HTML code into PDF images. To do that, there is some html clean-up I need to take care first: converting relative urls to absolute urls to preserve any image data; \n",
    "\n",
    "This is optional depending on how you convert them into PDFs or how much image matters to the downstream application. In this case, it is cheap enough so I am adding it here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a39c17427012783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_relative_url_to_absolute_url(base_url, html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for tag in soup.find_all(\"a\", href=True):\n",
    "        tag[\"href\"] = urljoin(base_url, tag[\"href\"])\n",
    "    for tag in soup.find_all(\"img\", src=True):\n",
    "        tag[\"src\"] = urljoin(base_url, tag[\"src\"])\n",
    "    return str(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13eb9d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_html_to_pdf(html):\n",
    "    doc = fitz.Document(stream=io.BytesIO(HTML(string=html).write_pdf()), filetype=\"pdf\")\n",
    "    images = [page.get_pixmap(dpi=120) for page in doc]\n",
    "    images = [Image.frombytes(\"RGB\", (image.width, image.height), image.samples) for image in images]\n",
    "    page_text = [page.get_text() for page in doc]\n",
    "\n",
    "    return page_text, images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6749e0a8e9b31eb",
   "metadata": {},
   "source": [
    "Now we are ready to do the actual conversion and extraction. Namely, I am extracting the following columns:\n",
    "1. Full text from the HTML document, which is used later for GPT-4o labelling;\n",
    "2. PDF image pages;\n",
    "3. Page text;\n",
    "4. Cleaned up HTML content for reproducibility;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9b9b82ea26eca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(record):\n",
    "    file_content = record[\"file_content\"]\n",
    "\n",
    "    if not file_content:\n",
    "        return {\"full_text\": \"\", \"images\": [], \"page_text\": [], \"html_content\": \"\"}\n",
    "\n",
    "    file_content_lower = file_content.lower()\n",
    "    if \"<html>\" not in file_content_lower or \"</html>\" not in file_content_lower:\n",
    "        return {\"full_text\": \"\", \"images\": [], \"page_text\": [], \"html_content\": \"\"}\n",
    "\n",
    "    left = file_content_lower.index(\"<html>\")\n",
    "    right = file_content_lower.index(\"</html>\")\n",
    "    html_content = file_content[left + len(\"<html>\") : right]\n",
    "\n",
    "    full_text = html2txt(html_content)\n",
    "    index_url = record[\"index_html_url\"]\n",
    "    base_url = index_url.replace(\"-index.html\", \"\").replace(\"-\", \"\") + \"/\"\n",
    "    html_content = convert_relative_url_to_absolute_url(base_url, html_content)\n",
    "    page_text, images = convert_html_to_pdf(html_content)\n",
    "\n",
    "    return {\n",
    "        \"full_text\": full_text,\n",
    "        \"images\": images,\n",
    "        \"page_text\": page_text,\n",
    "        \"html_content\": html_content,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfefd893242505",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "There are more than 800k documents in the current data repo. For the fine-tuning purpose, I am going to take sample documents that are uploaded this year so that:\n",
    "1. Documents are less likely to be scanned or of less quality assuming the natural progression and development of modern technology;\n",
    "2. Language used in those documents reflects more closely with current writing or contractual style;\n",
    "\n",
    "Some additional filtering rules are also applied here:\n",
    "1. Non empty text;\n",
    "2. Maximum 20 page of text;\n",
    "3. Minimum 50 characters in each page;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b1677797b5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = datetime.datetime(2024, 1, 1)\n",
    "sample = dataset[\"train\"].filter(lambda x: x[\"date\"] >= cutoff)\n",
    "sample = sample.map(lambda x: extract_text(x), num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5638623281a44ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sample = sample.filter(\n",
    "    lambda x: x[\"full_text\"]\n",
    "    and 1 <= len(x[\"page_text\"]) <= 20\n",
    "    and min(map(len, x[\"page_text\"])) >= 50,\n",
    "    num_proc=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66f7dccde446d0",
   "metadata": {},
   "source": [
    "### Key Information Extraction\n",
    "\n",
    "To compose a QA dataset, I need to come up with question and answer pairs somehow. Luckily, commercial large models are excellent at information extraction at this level, especially when you have access to the full text.\n",
    "\n",
    "You might ask: if you have the full text, isn't it easier to build a model directly on such data instead of modelling with images? It is true to some extent that text-based model can handle some of the QA tasks pretty well. But documents with a complex layout can pose a challenge that few text extraction or OCR tools can handle reasonably well.\n",
    "\n",
    "To create the dataset, I am going to ask GPT-4o to read the entire document full text and extract the key information with some help from langchain to enforce structured output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de985dac5925512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://support.ironcladapp.com/hc/en-us/articles/12947738534935-Ironclad-AI-Overview\n",
    "class KeyInformation(BaseModel):\n",
    "    agreement_date: str = Field(\n",
    "        description=\"Agreement signing date of the contract. (date)\"\n",
    "    )\n",
    "    effective_date: str = Field(description=\"Effective date of the contract. (date)\")\n",
    "    expiration_date: str = Field(\n",
    "        description=\"Service end date or expiration date of the contract. (date)\"\n",
    "    )\n",
    "    party_address: str = Field(description=\"Address of the party to the contract.\")\n",
    "    party_name: str = Field(description=\"The names of the contracting party.\")\n",
    "    counterparty_address: str = Field(\n",
    "        description=\"Address of the counterparty to the contract.\"\n",
    "    )\n",
    "    counterparty_name: str = Field(\n",
    "        description=\"The names of the contracting counterparty.\"\n",
    "    )\n",
    "    counterparty_signer_name: str = Field(\n",
    "        description=\"The name of the counterparty signer for each party to the agreement.\"\n",
    "    )\n",
    "    counterparty_signer_title: str = Field(\n",
    "        description=\"The counterparty signer’s title (e.g., CEO).\"\n",
    "    )\n",
    "    auto_renewal: str = Field(\n",
    "        description=\"Whether the contract term automatically renews (true/false).\"\n",
    "    )\n",
    "    governing_law: str = Field(description=\"(Jurisdiction) Choice of law.\")\n",
    "    venue: str = Field(\n",
    "        description=\"Location of the courts where legal proceedings will take place.\"\n",
    "    )\n",
    "    payment_frequency: str = Field(\n",
    "        description=\"The cadence for which payments are made (e.g., monthly, annually, one-time).\"\n",
    "    )\n",
    "    payment_term: str = Field(\n",
    "        description=\"When an invoice is due after issuance (e.g. Net 30)\"\n",
    "    )\n",
    "    renewal_term: str = Field(\n",
    "        description=\"The length of time the renewal period will last (e.g., 1 year, 2 years, 24 months etc.).\"\n",
    "    )\n",
    "    agreement_term: str = Field(\n",
    "        description=\"Term of the contract as an amount of time (e.g., 24 months).\"\n",
    "    )\n",
    "    termination_for_cause: str = Field(\n",
    "        description=\"Whether one or all parties may terminate the contract with cause, such as a breach of contract (true/false).\"\n",
    "    )\n",
    "    termination_for_convenience: str = Field(\n",
    "        description=\"Whether one or all parties may terminate the contract without cause, or at their convenience (true/false).\"\n",
    "    )\n",
    "    termination_notice_period: str = Field(\n",
    "        description=\"The period by which notice of termination must be given (e.g., 30 days).\"\n",
    "    )\n",
    "    opt_out_length: str = Field(\n",
    "        description=\"Required notice period to NOT renew (e.g., 30 days).\"\n",
    "    )\n",
    "    contract_value: str = Field(\n",
    "        description=\"Total fixed fee amount including currency codes or symbols. (monetary amount)\"\n",
    "    )\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "model = ChatOpenAI(model=\"gpt-4o\", max_retries=2).with_structured_output(KeyInformation)\n",
    "chain = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20caae5a425ae37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2qa(record):\n",
    "    page_text = record[\"page_text\"]\n",
    "    content = \"\\n\\n\".join(page_text)\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=\"\"\"You are a legal expert who is helping a client understand a contract. The client asks you to extract the key information for the given contract and return them in a structured format. Use N/A if not applicable or not available.\"\"\"\n",
    "        ),\n",
    "        HumanMessage(content=content),\n",
    "    ]\n",
    "\n",
    "    results = chain.invoke(messages)\n",
    "\n",
    "    return dict(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ee716c7d5a885",
   "metadata": {},
   "source": [
    "Since it is relatively expensive to call the model, I am going to use a simple loop to retry calling the endpoint if it failed for some reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177bce614f9a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = final_sample\n",
    "results = [None for _ in data]\n",
    "errors = list(range(len(data)))\n",
    "while errors:\n",
    "    new_errors = []\n",
    "    for i in tqdm.tqdm(errors):\n",
    "        try:\n",
    "            qa = text2qa(data[i])\n",
    "            results[i] = qa\n",
    "            time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error at {i}: {e}\")\n",
    "            new_errors.append(i)\n",
    "            continue\n",
    "\n",
    "    errors = new_errors\n",
    "    cont = input(f\"Found {len(errors)} errors. Continue? (y/n)\")\n",
    "    if cont.lower() != \"y\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc83c272da2eb2",
   "metadata": {},
   "source": [
    "### Saving the dataset\n",
    "\n",
    "The datasets is saved both locally and on Hugging Face Hub for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d34fc08a005902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "output = []\n",
    "for i, record in enumerate(final_sample):\n",
    "    output.append(record | results[i])\n",
    "\n",
    "df = pd.DataFrame(output)\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds.save_to_disk(\"temp-data\")\n",
    "ds.push_to_hub(\"chenghao/sec-material-contracts-qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d841d261498384",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "For this tutorial, I am going to use Lightning + transformers/peft to train the model. First, let me define some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60547b03c0ef1be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets peft bitsandbytes accelerate wandb \"git+https://github.com/huggingface/transformers\" autoawq lightning nltk loguru\n",
    "import os\n",
    "import re\n",
    "\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "from loguru import logger\n",
    "from datasets import concatenate_datasets\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from nltk import edit_distance\n",
    "from peft import LoraConfig\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoProcessor\n",
    "from transformers import BitsAndBytesConfig, Idefics2ForConditionalGeneration\n",
    "try:\n",
    "    from deepspeed.ops.adam import FusedAdam\n",
    "except ImportError:\n",
    "    from torch.optim import Adam as FusedAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9b2986",
   "metadata": {},
   "source": [
    "A helper function to convert the dataset to the QA format that Idefics2 can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b3d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_key_info_to_qa(records: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Convert key information into QA format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    records : dict\n",
    "        The records from the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The QA format of the records.\n",
    "    \"\"\"\n",
    "    key2question = {\n",
    "        \"agreement_date\": \"When is the signing date of this agreement?\",\n",
    "        \"effective_date\": \"When is the effective date of the contract?\",\n",
    "        \"expiration_date\": \"When is the service end date or expiration date of the contract?\",\n",
    "        \"party_address\": \"What is the address of the party to the contract?\",\n",
    "        \"party_name\": \"What are the names of the contracting party?\",\n",
    "        \"counterparty_address\": \"What is the address of the counterparty to the contract?\",\n",
    "        \"counterparty_name\": \"What are the names of the contracting counterparty?\",\n",
    "        \"counterparty_signer_name\": \"What is the name of the counterparty signer for each party to the agreement?\",\n",
    "        \"counterparty_signer_title\": \"What is the counterparty signer’s title?\",\n",
    "        \"auto_renewal\": \"Whether the contract term automatically renews (true/false).\",\n",
    "        \"governing_law\": \"Where is the jurisdiction or choice of law?\",\n",
    "        \"venue\": \"where is the location of the courts where legal proceedings will take place?\",\n",
    "        \"payment_frequency\": \"what is the cadence for which payments are made (e.g., monthly, annually, one-time)?\",\n",
    "        \"payment_term\": \"When an invoice is due after issuance (e.g. Net 30)?\",\n",
    "        \"renewal_term\": \"What is the length of time the renewal period will last (e.g., 1 year, 2 years, 24 months etc.)?\",\n",
    "        \"agreement_term\": \"What is the term of the contract as an amount of time (e.g., 24 months)?\",\n",
    "        \"termination_for_cause\": \"Whether one or all parties may terminate the contract with cause, such as a breach of contract (true/false).\",\n",
    "        \"termination_for_convenience\": \"Whether one or all parties may terminate the contract without cause, or at their convenience (true/false).\",\n",
    "        \"termination_notice_period\": \"What is the period by which notice of termination must be given (e.g., 30 days)?\",\n",
    "        \"opt_out_length\": \"What is the required notice period to NOT renew (e.g., 30 days)?\",\n",
    "        \"contract_value\": \"What is the total fixed fee amount including currency codes or symbols?\",\n",
    "    }\n",
    "    images = records[\"images\"][0]\n",
    "    questions, answers = [], []\n",
    "    for key in key2question:\n",
    "        answer = records[key][0]\n",
    "        if answer != \"N/A\":\n",
    "            questions.append(key2question[key])\n",
    "            answers.append(answer)\n",
    "\n",
    "    output = {\n",
    "        \"images\": [images for _ in questions],\n",
    "        \"answer\": answers,\n",
    "        \"question\": questions\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e3aa3f",
   "metadata": {},
   "source": [
    "Create a custom dataset for Idefics2 training. Specifically, we can specify a maximum page number for each document so that we can control the token length for each document. Here, I am using both the head and tail of the document and skipping the middle part. This is based on the assumption that the middle part is the most template-like part of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c43f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Idefics2Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Idefics2. This class takes a HuggingFace Dataset as input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_page):\n",
    "        super().__init__()\n",
    "        self.dataset = ds\n",
    "        self.max_page = max_page\n",
    "        assert max_page > 0\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        if len(sample[\"images\"]) > self.max_page:\n",
    "            images = sample[\"images\"][:self.max_page // 2] + sample[\"images\"][-self.max_page // 2:]\n",
    "        else:\n",
    "            images = sample[\"images\"]\n",
    "\n",
    "        question = sample[\"question\"]\n",
    "        answer = sample[\"answer\"]\n",
    "\n",
    "        return images, question, answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb7db0",
   "metadata": {},
   "source": [
    "For a custom dataset, it is important to create custom collate function to handle the mini batching. The idea is to format each example into the training format by concatenating the images and the question, pad the input according to the maximum length, and then return the input ids, attention mask, pixel values, and pixel attention mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e947338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collate_fn(examples):\n",
    "    global processor\n",
    "    global image_token_id\n",
    "\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        images_example, question, answer = example\n",
    "        content = [{\"type\": \"image\"} for _ in range(len(images_example))]\n",
    "        content += [{\"type\": \"text\", \"text\": question}]\n",
    "\n",
    "        # Create inputs\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": answer},\n",
    "                ]\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        prompt = processor.apply_chat_template(messages, add_generation_prompt=False)\n",
    "        texts.append(prompt)\n",
    "        images.append(images_example)\n",
    "\n",
    "    batch = processor(text=texts, images=images, padding=True, truncation=True, max_length=MAX_LENGTH,\n",
    "                      return_tensors=\"pt\")\n",
    "\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    pixel_attention_mask = batch[\"pixel_attention_mask\"]\n",
    "    labels = batch[\"labels\"].long()\n",
    "\n",
    "    return input_ids, attention_mask, pixel_values, pixel_attention_mask, labels\n",
    "\n",
    "\n",
    "def eval_collate_fn(examples):\n",
    "    images = []\n",
    "    texts = []\n",
    "    answers = []\n",
    "    for example in examples:\n",
    "        images_example, question, answer = example\n",
    "\n",
    "        content = [{\"type\": \"image\"} for _ in range(len(images_example))]\n",
    "        content += [{\"type\": \"text\", \"text\": question}]\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content,\n",
    "            },\n",
    "        ]\n",
    "        text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        images.append(images_example)\n",
    "        texts.append(text.strip())\n",
    "        answers.append(answer)\n",
    "\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    pixel_attention_mask = batch[\"pixel_attention_mask\"]\n",
    "\n",
    "    return input_ids, attention_mask, pixel_values, pixel_attention_mask, answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa185fd26459bc9",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "Along with the model definition, I also included the callback from the original tutorial so that model can be pushed to the hub after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a9d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Idefics2ModelPLModule(L.LightningModule):\n",
    "    def __init__(self, model_config, inp_processor, torch_model):\n",
    "        super().__init__()\n",
    "        self.config = model_config\n",
    "        self.processor = inp_processor\n",
    "        self.model = torch_model\n",
    "        self.batch_size = self.config.get(\"batch_size\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, pixel_values, pixel_attention_mask, labels = batch\n",
    "        outputs = self.model(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             pixel_values=pixel_values,\n",
    "                             pixel_attention_mask=pixel_attention_mask,\n",
    "                             labels=labels\n",
    "                             )\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "        global MAX_LENGTH\n",
    "\n",
    "        input_ids, attention_mask, pixel_values, pixel_attention_mask, answers = batch\n",
    "\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "            pixel_attention_mask=pixel_attention_mask,\n",
    "            max_new_tokens=MAX_LENGTH\n",
    "        )\n",
    "        # turn them back into text, chopping of the prompt\n",
    "        # important: we don't skip special tokens here, because we want to see them in the output\n",
    "        predictions = self.processor.batch_decode(generated_ids[:, input_ids.size(1):], skip_special_tokens=True)\n",
    "\n",
    "        scores = []\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            pred = re.sub(r\"((?<=>) | (?=</s_))\", \"\", pred)\n",
    "            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n",
    "\n",
    "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
    "                logger.debug(f\"\"\"\n",
    "Prediction: `{pred}`\n",
    "Answer: `{answer}`\n",
    "Normed ED: `{scores[0]:.3f}`\"\"\")\n",
    "\n",
    "        self.log(\"val_edit_distance\", np.mean(scores))\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # you could also add a learning rate scheduler if you want\n",
    "        optimizer = FusedAdam(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        global train_dataset\n",
    "        return DataLoader(train_dataset, collate_fn=train_collate_fn, batch_size=self.batch_size, shuffle=True,\n",
    "                          num_workers=6)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        global validation_dataset\n",
    "        return DataLoader(validation_dataset, collate_fn=eval_collate_fn, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=6)\n",
    "\n",
    "\n",
    "class PushToHubCallback(Callback):\n",
    "\n",
    "    def on_train_epoch_end(self, pl_trainer, pl_module):\n",
    "        global FINETUNED_REPO_ID\n",
    "        logger.info(f\"Pushing model to the hub, epoch {pl_trainer.current_epoch}\")\n",
    "        pl_module.model.push_to_hub(\n",
    "            FINETUNED_REPO_ID,\n",
    "            commit_message=f\"Training in progress, epoch {pl_trainer.current_epoch}\"\n",
    "        )\n",
    "\n",
    "    def on_train_end(self, pl_trainer, pl_module):\n",
    "        global FINETUNED_REPO_ID\n",
    "        logger.info(\"Pushing model to the hub after training\")\n",
    "        pl_module.processor.push_to_hub(FINETUNED_REPO_ID, commit_message=\"Training done\")\n",
    "        pl_module.model.push_to_hub(FINETUNED_REPO_ID, commit_message=\"Training done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1db8e",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "To start training, I am using the following configuration:\n",
    "\n",
    "1. QLoRA\n",
    "2. Maximum page size of 5\n",
    "3. Batch size of 2 with gradient accumulation of 12\n",
    "4. Learning rate of 1e-4\n",
    "5. Number of epochs of 10\n",
    "6. Gradient clipping of 1.0\n",
    "7. Adam optimizer (or FusedAdam) \n",
    "8. Maximum length of 1024 tokens\n",
    "9. Warmup steps of 50\n",
    "10. No image splitting so only 64 tokens per image\n",
    "11. Reduced image resolution to maximum of 350 * 490 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beffedb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 1024\n",
    "USE_LORA = False\n",
    "USE_QLORA = True\n",
    "MAX_PAGE = 5\n",
    "FINETUNED_REPO_ID = \"chenghao/idefics2-edgar\"\n",
    "WANDB_PROJECT = \"Idefics2-EDGAR\"\n",
    "WANDB_NAME = \"demo-run\"\n",
    "config = {\n",
    "    \"max_epochs\": 10,\n",
    "    # \"val_check_interval\": 0.2,\n",
    "    \"check_val_every_n_epoch\": 1,\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    \"accumulate_grad_batches\": 12,\n",
    "    \"lr\": 1e-4,\n",
    "    \"batch_size\": 2,\n",
    "    \"precision\": \"16-mixed\",\n",
    "    \"seed\": 42,\n",
    "    \"warmup_steps\": 50,\n",
    "    \"result_path\": \"./result\",\n",
    "    \"verbose\": True,\n",
    "}\n",
    "\n",
    "wandb_logger = WandbLogger(project=WANDB_PROJECT, name=WANDB_NAME)\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"HuggingFaceM4/idefics2-8b\",\n",
    "    do_image_splitting=False,\n",
    "    size={\"longest_edge\": 490, \"shortest_edge\": 350}\n",
    ")\n",
    "image_token_id = processor.tokenizer.additional_special_tokens_ids[processor.tokenizer.additional_special_tokens.index(\"<image>\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0344b2ec",
   "metadata": {},
   "source": [
    "Split the dataset into train and validation set if not done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd939f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"local-dataset\"):\n",
    "    dataset = load_from_disk(\"local-dataset\")\n",
    "else:\n",
    "    dude_dataset = load_dataset(\"jordyvl/DUDE_subset_100val\")\n",
    "    edgar_dataset = load_dataset(\"chenghao/sec-material-contracts-qa\")\n",
    "    flattened_edgar_dataset = edgar_dataset['train'].map(\n",
    "        convert_key_info_to_qa, batched=True, batch_size=1,\n",
    "        remove_columns=edgar_dataset['train'].column_names, num_proc=10)\n",
    "    # flattened_edgar_dataset = flattened_edgar_dataset.filter(lambda x: len(x['images']) <= MAX_PAGE, num_proc=10)\n",
    "    dude_dataset = dude_dataset.remove_columns([\"questionId\"])\n",
    "    flattened_edgar_dataset = flattened_edgar_dataset.cast(dude_dataset['train'].features)\n",
    "    all_dataset = concatenate_datasets([dude_dataset['train'], flattened_edgar_dataset])\n",
    "    dataset = all_dataset.train_test_split(test_size=0.2)\n",
    "    dataset.save_to_disk(\"local-dataset\")\n",
    "\n",
    "train_dataset = Idefics2Dataset(dataset[\"train\"], max_page=MAX_PAGE)\n",
    "validation_dataset = Idefics2Dataset(dataset[\"test\"], max_page=MAX_PAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bfd64a",
   "metadata": {},
   "source": [
    "Load the model according to the configurations.\n",
    "\n",
    "QLoRA essentially means quantization + LoRA, which is a technique that allows you to fine-tune a pre-trained model on a dataset with low-precision data types, such as 8-bit or 4-bit quantization and low-rank adaptation (LoRA) techniques. This technique can significantly reduce the memory footprint of the model and improve its performance for training on low-precision hardware.\n",
    "\n",
    "Here, I am using 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights, and double quantization to reduce the average memory footprint by quantizing the quantization constants. For the LoRA part, I am using rank 8 and a dropout rate of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d13753",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_QLORA or USE_LORA:\n",
    "    if USE_QLORA:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "    else:\n",
    "        quantization_config = None\n",
    "\n",
    "    model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "        \"HuggingFaceM4/idefics2-8b\",\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=quantization_config,\n",
    "    )\n",
    "else:\n",
    "    # for full fine-tuning, we can speed up the model using Flash Attention\n",
    "    # only available on certain devices, see\n",
    "    # https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features\n",
    "    model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "        \"HuggingFaceM4/idefics2-8b\",\n",
    "        torch_dtype=torch.float16,\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "\n",
    "if USE_QLORA or USE_LORA:\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=\".*(text_model|modality_projection|perceiver_resampler).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$\",\n",
    "        init_lora_weights=\"gaussian\",\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_module = Idefics2ModelPLModule(config, processor, model)\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_edit_distance\", patience=3, verbose=False, mode=\"min\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    strategy=\"deepspeed_stage_2\",\n",
    "    max_epochs=config.get(\"max_epochs\"),\n",
    "    check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
    "    gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
    "    accumulate_grad_batches=config.get(\"accumulate_grad_batches\"),\n",
    "    precision=config.get(\"precision\"),\n",
    "    num_sanity_val_steps=10,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[PushToHubCallback(), early_stop_callback],\n",
    ")\n",
    "\n",
    "trainer.fit(model_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a93a12",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dd6a5b",
   "metadata": {},
   "source": [
    "Next, I am going to load the model from the hub and evaluate it on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f55347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, Idefics2ForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "base_model = \"HuggingFaceM4/idefics2-8b\"\n",
    "peft_model_id = \"chenghao/idefics2-edgar\"\n",
    "# peft_model_id = \"HuggingFaceM4/idefics2-8b\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "    peft_model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    base_model,\n",
    "    do_image_splitting=False,\n",
    "    size={\"longest_edge\": 490, \"shortest_edge\": 350}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e766d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def evaluate():\n",
    "    \n",
    "    questions = []\n",
    "    answers = []\n",
    "    predictions = []\n",
    "    \n",
    "    for example in tqdm(ds[\"test\"]):\n",
    "        images = example[\"images\"][:2] + example[\"images\"][-2:]\n",
    "        question, answer = example[\"question\"], example[\"answer\"]\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"image\"} for _ in range(len(images))] + [{\"type\": \"text\", \"text\": question}],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        inputs = processor(text=prompt, images=images, return_tensors=\"pt\").to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "        generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        preds = [t.split(\"Assistant:\", 1)[-1].strip() for t in generated_texts]\n",
    "\n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        predictions.append(preds[0])\n",
    "    \n",
    "    return questions, answers, predictions\n",
    "\n",
    "questions, answers, predictions = evaluate()\n",
    "# ! Save them somewhere for later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31322b18",
   "metadata": {},
   "source": [
    "For the evaluation, I am using the edit distance to measure the similarity between the predicted answer and the ground truth answer. The edit distance is calculated as the number of insertions, deletions, and substitutions required to transform the predicted answer into the ground truth answer. The edit distance is then normalized by the length of the longest answer and the shortest answer to get a score between 0 and 1. \n",
    "\n",
    "The mean of all the edit distance scores is used as the evaluation metric (either by label (macro) or by question (micro))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cada9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import edit_distance\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "key2question = {\n",
    "    \"agreement_date\": \"When is the signing date of this agreement?\",\n",
    "    \"effective_date\": \"When is the effective date of the contract?\",\n",
    "    \"expiration_date\": \"When is the service end date or expiration date of the contract?\",\n",
    "    \"party_address\": \"What is the address of the party to the contract?\",\n",
    "    \"party_name\": \"What are the names of the contracting party?\",\n",
    "    \"counterparty_address\": \"What is the address of the counterparty to the contract?\",\n",
    "    \"counterparty_name\": \"What are the names of the contracting counterparty?\",\n",
    "    \"counterparty_signer_name\": \"What is the name of the counterparty signer for each party to the agreement?\",\n",
    "    \"counterparty_signer_title\": \"What is the counterparty signer’s title?\",\n",
    "    \"auto_renewal\": \"Whether the contract term automatically renews (true/false).\",\n",
    "    \"governing_law\": \"Where is the jurisdiction or choice of law?\",\n",
    "    \"venue\": \"where is the location of the courts where legal proceedings will take place?\",\n",
    "    \"payment_frequency\": \"what is the cadence for which payments are made (e.g., monthly, annually, one-time)?\",\n",
    "    \"payment_term\": \"When an invoice is due after issuance (e.g. Net 30)?\",\n",
    "    \"renewal_term\": \"What is the length of time the renewal period will last (e.g., 1 year, 2 years, 24 months etc.)?\",\n",
    "    \"agreement_term\": \"What is the term of the contract as an amount of time (e.g., 24 months)?\",\n",
    "    \"termination_for_cause\": \"Whether one or all parties may terminate the contract with cause, such as a breach of contract (true/false).\",\n",
    "    \"termination_for_convenience\": \"Whether one or all parties may terminate the contract without cause, or at their convenience (true/false).\",\n",
    "    \"termination_notice_period\": \"What is the period by which notice of termination must be given (e.g., 30 days)?\",\n",
    "    \"opt_out_length\": \"What is the required notice period to NOT renew (e.g., 30 days)?\",\n",
    "    \"contract_value\": \"What is the total fixed fee amount including currency codes or symbols?\",\n",
    "}\n",
    "question2key = {q: k for k, q in key2question.items()}\n",
    "\n",
    "def calculate_edit_distance(questions, answers, predictions):\n",
    "    scores = defaultdict(list)\n",
    "    for question, pred, answer in tqdm(zip(questions, predictions, answers), total=len(answers)):\n",
    "        if question not in question2key:\n",
    "            continue\n",
    "        question = question2key[question]\n",
    "        scores[question].append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n",
    "    return {question: np.mean(values) for question, values in scores.items()}, np.mean([v for values in scores.values() for v in values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cab9c8",
   "metadata": {},
   "source": [
    "Plotting the results with a bar chart with the results saved from the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb70ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "def calculate_df(file1, file2):\n",
    "    \n",
    "    with open(file1) as f:\n",
    "        d1 = json.load(f)\n",
    "        r1, a1 = calculate_edit_distance(d1['questions'], d1['answers'], d1['predictions'])\n",
    "    \n",
    "    \n",
    "    with open(file2) as f:\n",
    "        d2 = json.load(f)\n",
    "        r2, a2 = calculate_edit_distance(d2['questions'], d2['answers'], d2['predictions'])\n",
    "    \n",
    "    print(f\"{(a1-a2)/a1*100:.2f}%\")\n",
    "    records = []\n",
    "    name = {\n",
    "        \"base\": \"Idefics2-8B\",\n",
    "        \"finetuned\": \"Idefics2-8B-EDGAR\"\n",
    "    }\n",
    "    for key, value in r1.items():\n",
    "        n = name[file1.replace(\".json\", \"\")]\n",
    "        records.append((key, value, n))\n",
    "    for key, value in r2.items():\n",
    "        n = name[file2.replace(\".json\", \"\")]\n",
    "        records.append((key, value, n))\n",
    "    \n",
    "    return pd.DataFrame(records, columns=[\"category\", \"value\", \"model\"])\n",
    "\n",
    "\n",
    "sns.set_theme(font_scale=1.4)\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "df = calculate_df(\"base.json\", \"finetuned.json\")\n",
    "sns.barplot(y=df.category, x=df.value, hue=df.model, ax=ax)\n",
    "ax.set_xlabel(\"Average Edit Distance (lower is better)\")\n",
    "ax.set_ylabel(\"Category\")\n",
    "\n",
    "ax.set_title(\"Comparison between base and finetuned model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12301df4",
   "metadata": {},
   "source": [
    "A more detailed breakdown of the differences between the two models for each category can be found in the following table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for category, row in df.groupby(\"category\"):\n",
    "    base = row[row[\"model\"] == \"Idefics2-8B\"][\"value\"].iloc[0]\n",
    "    after = row[row[\"model\"] != \"Idefics2-8B\"][\"value\"].iloc[0]\n",
    "    delta = (base - after) / base\n",
    "    data.append((category, base, after, f\"{delta * 100:.2f}%\"))\n",
    "\n",
    "print(pd.DataFrame(data, columns=[\"Category\", \"Idefics2-8B\", \"Idefics2-8B-EDGAR\", \"Δ(↑)\"]).to_markdown())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
